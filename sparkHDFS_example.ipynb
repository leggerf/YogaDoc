{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Spark context\n",
    "\n",
    "We use the predefined custom magic *%sc* to load a pre-defined Spark context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=k8s://https://192.168.2.39:6443 appName=jupyter-leggerf>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-leggerf.jhub.svc.cluster.local:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://192.168.2.39:6443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>jupyter-leggerf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=k8s://https://192.168.2.39:6443 appName=jupyter-leggerf>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark=%sc\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession(spark)\n",
    "\n",
    "#check if spark is there\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and write from HDFS with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.9 ms, sys: 22.5 ms, total: 98.5 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "# read from HDFS shared area\n",
    "inputFile = 'hdfs://192.168.2.39/data/Higgs1M.csv'\n",
    "\n",
    "%time df = spark_session.read.format('csv').option('header', 'true').option('inferschema', 'true').load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to your own (path: user/YOURUSERNAME/) HDFS area\n",
    "\n",
    "hdfs_path = 'hdfs://192.168.2.39/user/leggerf'\n",
    "outputFile = hdfs_path+'/Higgs1M.parquet'\n",
    "\n",
    "df.write.parquet(outputFile, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.1 ms, sys: 20.2 ms, total: 87.3 ms\n",
      "Wall time: 4.34 s\n",
      "There are 999999 events\n"
     ]
    }
   ],
   "source": [
    "%time total_events = df.count()\n",
    "\n",
    "print('There are '+str(total_events)+' events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- lepton_pT: double (nullable = true)\n",
      " |-- lepton_eta: double (nullable = true)\n",
      " |-- lepton_phi: double (nullable = true)\n",
      " |-- missing_energy_magnitude: double (nullable = true)\n",
      " |-- missing_energy_phi: double (nullable = true)\n",
      " |-- jet1_pt: double (nullable = true)\n",
      " |-- jet1_eta: double (nullable = true)\n",
      " |-- jet1_phi: double (nullable = true)\n",
      " |-- jet1_b-tag: double (nullable = true)\n",
      " |-- jet2_pt: double (nullable = true)\n",
      " |-- jet2_eta: double (nullable = true)\n",
      " |-- jet2_phi: double (nullable = true)\n",
      " |-- jet2_b-tag: double (nullable = true)\n",
      " |-- jet3_pt: double (nullable = true)\n",
      " |-- jet3_eta: double (nullable = true)\n",
      " |-- jet3_phi: double (nullable = true)\n",
      " |-- jet3_b-tag: double (nullable = true)\n",
      " |-- jet4_pt: double (nullable = true)\n",
      " |-- je4_eta: double (nullable = true)\n",
      " |-- jet4_phi: double (nullable = true)\n",
      " |-- jet4_b-tag: double (nullable = true)\n",
      " |-- m_jj: double (nullable = true)\n",
      " |-- m_jjj: double (nullable = true)\n",
      " |-- m_lv: double (nullable = true)\n",
      " |-- m_jlv: double (nullable = true)\n",
      " |-- m_bb: double (nullable = true)\n",
      " |-- m_wbb: double (nullable = true)\n",
      " |-- m_wwbb: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when you're done, stop spark, this will release the resources you're using\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.driver.maxResultSize",
     "value": "0"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
